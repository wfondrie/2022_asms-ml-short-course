{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Links to Lab Exercises Day 1 Lab 1: Getting Started with Machine Learning Lab 2: Dimensionality Reduction Lab 3: Clustering Lab 4: Model Evaluation Day 2 Lab 5: Logistic Regression Lab 6: Support Vector Machines Lab 7: Random Forest Lab 8: Neural Networks","title":"Home"},{"location":"#links-to-lab-exercises","text":"","title":"Links to Lab Exercises"},{"location":"#day-1","text":"Lab 1: Getting Started with Machine Learning Lab 2: Dimensionality Reduction Lab 3: Clustering Lab 4: Model Evaluation","title":"Day 1"},{"location":"#day-2","text":"Lab 5: Logistic Regression Lab 6: Support Vector Machines Lab 7: Random Forest Lab 8: Neural Networks","title":"Day 2"},{"location":"2_dimensionality_reduction/","text":"Lab 2: Dimensionality Reduction Dimensionality reduction is often used during the exploratory data analysis (EDA) phase. Understanding the characteristics of your data is the most important part in building a successful machine learning model! The type of model and its hyperparameters are only a secondary aspect. Exercise 1: Explore public metabolomics data ReDU is a system to annotate public metabolomics data on the GNPS repository with metadata information. This metadata captures various relevant pieces of information: from which organism the sample was derived, when and where the data was collected, data acquisition settings, etc. Public data on GNPS was processed using spectral library searching, and the unique metabolite annotations were used as features. Thus, our data table consists of the unique metabolites in the columns and the file names in the rows, with 1 if the metabolite was detected in that file, and 0 otherwise. Because there are hundreds of thousands of unique metabolites that can be annotated using the GNPS spectral libraries, this data table cannot be visualized directly. Instead, PCA was used to project the data into two or three dimensions for visualization purposes. Go to the ReDU website and start the PCA exploration by clicking on \u201cExplore Multivariate Analysis of Public Data\u201d. This will bring up an interactive viewer where each dot represents a single mass spectrometry file. What can you learn about the chemical similarity of files with ReDU information? Explore the PCA data. Color the files by different metadata categories. Can you find any patterns? Rotate the axes to explore subspaces of the data. Compare a two-dimensional plot to a three-dimensional plot. What do you prefer? What do the percentages next to the axis labels mean? Are these values good or bad? Exercise 2: Compare dimensionality reduction techniques Which dimensionality reduction technique is best? There is no definitive answer, each techniques has its own assumptions, strengths, and weaknesses. Here we\u2019ll explore the difference between PCA and t-SNE using the MNIST dataset. MNIST is a large database of handwritten digits that is often used as a simple benchmark dataset to evaluate image processing systems. Open the Tensorflow Embedding Projector website. This website provides a browser interface to implementations of a few common embedding algorithms (PCA, t-SNE, UMAP). An embedding is a low-dimensional space into which you can translate high-dimensional vectors. MNIST images are 28x28 pixels, thus they can be represented as 784 dimensional vectors. (Q: How did we get this number?) For visualization purposes, we transform the original 784 dimensional data into embeddings of two or three dimensions. Compare the dimensionality reductions achieved by PCA and t-SNE. Select \u201cMnist with images\u201d in the drop-down menu in the top left corner. Choose \u201clabel\u201d as the \u201cColor by\u201d value. Explore the PCA embeddings . Rotate and drag the data projection. Change between two and three dimensions (bottom left). Which digits occur close to each other? Why? What does \u201cTotal variance described\u201d (bottom left) mean? Why does this value change when you switch between two and three components? Is PCA a good embedding? Why (not)? Switch to t-SNE embeddings . Is there a runtime difference between PCA and t-SNE? Observe how the embeddings evolve as t-SNE is trained for multiple generations. Do you see unexpected clustering of the data? For example, how does t-SNE group the digit 1 ? Try different hyperparameters. What are the effects of the perplexity and learning rate hyperparameters? Exercise 3 (bonus): Explore t-SNE hyperparameters The performance of t-SNE can be heavily influenced by its hyperparameters. Additionally, because t-SNE uses a non-deterministic optimization procedure, its results can differ even with the same hyperparameters (unlike for PCA). Suboptimal t-SNE hyperparameters might suggest a data clustering that is not present in the full-dimensional data. The Distill article \u201cHow to use t-SNE effectively?\u201d allows you to interactively explore the effect of different t-SNE hyperparameters on several toy datasets. (The hyperparameter \u201cepsilon\u201d is the same as the \u201clearning rate\u201d in the previous exercise.) Play around with t-SNE hyperparameters. Select different datasets and hyperparameter combinations. Let t-SNE run until convergence. Re-run t-SNE with unchanged hyperparameters. Is the embedding identical? Did you find typical failure cases of t-SNE? What is the influence of the perplexity hyperparameter?","title":"Lab 2: Dimensionality Reduction"},{"location":"2_dimensionality_reduction/#lab-2-dimensionality-reduction","text":"Dimensionality reduction is often used during the exploratory data analysis (EDA) phase. Understanding the characteristics of your data is the most important part in building a successful machine learning model! The type of model and its hyperparameters are only a secondary aspect.","title":"Lab 2: Dimensionality Reduction"},{"location":"2_dimensionality_reduction/#exercise-1-explore-public-metabolomics-data","text":"ReDU is a system to annotate public metabolomics data on the GNPS repository with metadata information. This metadata captures various relevant pieces of information: from which organism the sample was derived, when and where the data was collected, data acquisition settings, etc. Public data on GNPS was processed using spectral library searching, and the unique metabolite annotations were used as features. Thus, our data table consists of the unique metabolites in the columns and the file names in the rows, with 1 if the metabolite was detected in that file, and 0 otherwise. Because there are hundreds of thousands of unique metabolites that can be annotated using the GNPS spectral libraries, this data table cannot be visualized directly. Instead, PCA was used to project the data into two or three dimensions for visualization purposes. Go to the ReDU website and start the PCA exploration by clicking on \u201cExplore Multivariate Analysis of Public Data\u201d. This will bring up an interactive viewer where each dot represents a single mass spectrometry file. What can you learn about the chemical similarity of files with ReDU information? Explore the PCA data. Color the files by different metadata categories. Can you find any patterns? Rotate the axes to explore subspaces of the data. Compare a two-dimensional plot to a three-dimensional plot. What do you prefer? What do the percentages next to the axis labels mean? Are these values good or bad?","title":"Exercise 1: Explore public metabolomics data"},{"location":"2_dimensionality_reduction/#exercise-2-compare-dimensionality-reduction-techniques","text":"Which dimensionality reduction technique is best? There is no definitive answer, each techniques has its own assumptions, strengths, and weaknesses. Here we\u2019ll explore the difference between PCA and t-SNE using the MNIST dataset. MNIST is a large database of handwritten digits that is often used as a simple benchmark dataset to evaluate image processing systems. Open the Tensorflow Embedding Projector website. This website provides a browser interface to implementations of a few common embedding algorithms (PCA, t-SNE, UMAP). An embedding is a low-dimensional space into which you can translate high-dimensional vectors. MNIST images are 28x28 pixels, thus they can be represented as 784 dimensional vectors. (Q: How did we get this number?) For visualization purposes, we transform the original 784 dimensional data into embeddings of two or three dimensions. Compare the dimensionality reductions achieved by PCA and t-SNE. Select \u201cMnist with images\u201d in the drop-down menu in the top left corner. Choose \u201clabel\u201d as the \u201cColor by\u201d value. Explore the PCA embeddings . Rotate and drag the data projection. Change between two and three dimensions (bottom left). Which digits occur close to each other? Why? What does \u201cTotal variance described\u201d (bottom left) mean? Why does this value change when you switch between two and three components? Is PCA a good embedding? Why (not)? Switch to t-SNE embeddings . Is there a runtime difference between PCA and t-SNE? Observe how the embeddings evolve as t-SNE is trained for multiple generations. Do you see unexpected clustering of the data? For example, how does t-SNE group the digit 1 ? Try different hyperparameters. What are the effects of the perplexity and learning rate hyperparameters?","title":"Exercise 2: Compare dimensionality reduction techniques"},{"location":"2_dimensionality_reduction/#exercise-3-bonus-explore-t-sne-hyperparameters","text":"The performance of t-SNE can be heavily influenced by its hyperparameters. Additionally, because t-SNE uses a non-deterministic optimization procedure, its results can differ even with the same hyperparameters (unlike for PCA). Suboptimal t-SNE hyperparameters might suggest a data clustering that is not present in the full-dimensional data. The Distill article \u201cHow to use t-SNE effectively?\u201d allows you to interactively explore the effect of different t-SNE hyperparameters on several toy datasets. (The hyperparameter \u201cepsilon\u201d is the same as the \u201clearning rate\u201d in the previous exercise.) Play around with t-SNE hyperparameters. Select different datasets and hyperparameter combinations. Let t-SNE run until convergence. Re-run t-SNE with unchanged hyperparameters. Is the embedding identical? Did you find typical failure cases of t-SNE? What is the influence of the perplexity hyperparameter?","title":"Exercise 3 (bonus): Explore t-SNE hyperparameters"},{"location":"8_neural_networks/","text":"Lab 8: Neural Networks In this final lab, we will play around with simple multi-layer neural networks using the Neural Network Playground . The Playground allows is to vary the neural network architecture (number of layers, number of neurons per layer) and other hyperparameters (learning rate, activation function, etc.). Play around with the different options. Next, try to answer the following questions. Take notes to remember what happened, so that we can discuss this together. What happens when you select a very small or very large learning rate? Add different numbers of hidden layers and different numbers of neurons per hidden layer. Compare the performance of shallow and wide neural networks to the performance of deep and narrow neural networks. Which one performs best? Look at the outputs learned by intermediate neurons. Remove the hidden layers so that you have 0 hidden layers. What kind of decision function can this model learn? What would we call this model? Use the \u201cGaussian\u201d dataset with a deep neural network and increase the noise level. The network will start to overfit to the data. What can we do to combat overfitting? (Deep) neural networks can approximate complex data distributions. Investigate the training evolution on the \u201cSwiss roll\u201d dataset (bottom right). Create a logistic regression model for the \u201ccircle\u201d dataset. Can we learn a non-linear decision function? Add non-linear features (x\u2081\u00b2 and x\u2082\u00b2). How does the decision function change? Try to find similarly suitable non-linear features for the \u201cexclusive or\u201d dataset. Modify your logistic regression model to a multi-layer neural network and remove the non-linear features. Can we still accurately predict this dataset?","title":"Lab 8: Neural Networks"},{"location":"8_neural_networks/#lab-8-neural-networks","text":"In this final lab, we will play around with simple multi-layer neural networks using the Neural Network Playground . The Playground allows is to vary the neural network architecture (number of layers, number of neurons per layer) and other hyperparameters (learning rate, activation function, etc.). Play around with the different options. Next, try to answer the following questions. Take notes to remember what happened, so that we can discuss this together. What happens when you select a very small or very large learning rate? Add different numbers of hidden layers and different numbers of neurons per hidden layer. Compare the performance of shallow and wide neural networks to the performance of deep and narrow neural networks. Which one performs best? Look at the outputs learned by intermediate neurons. Remove the hidden layers so that you have 0 hidden layers. What kind of decision function can this model learn? What would we call this model? Use the \u201cGaussian\u201d dataset with a deep neural network and increase the noise level. The network will start to overfit to the data. What can we do to combat overfitting? (Deep) neural networks can approximate complex data distributions. Investigate the training evolution on the \u201cSwiss roll\u201d dataset (bottom right). Create a logistic regression model for the \u201ccircle\u201d dataset. Can we learn a non-linear decision function? Add non-linear features (x\u2081\u00b2 and x\u2082\u00b2). How does the decision function change? Try to find similarly suitable non-linear features for the \u201cexclusive or\u201d dataset. Modify your logistic regression model to a multi-layer neural network and remove the non-linear features. Can we still accurately predict this dataset?","title":"Lab 8: Neural Networks"}]}